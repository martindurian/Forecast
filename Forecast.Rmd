---
title: "Energy consumption estimation"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE, warning = FALSE, message=FALSE)
```


## Data preparation

```{r load section}
library(tidyverse)
library(tsibble)
library(lubridate)
library(feasts) #tsibbledata, feasts, fable
library(cowplot)
library(ggwrap)
library(fable)
library(fable.prophet)
library(fabletools)

source("C:/Users/sk0007/OneDrive - Daikin Europe N.V/Documents/R scripts/DCS/DCS/MasterSctripts/Various_data_structures.R", encoding = "UTF-8")
# working_path <- "C:/Users/sk0007/OneDrive - Daikin Europe N.V/Documents/R scripts/DCS/DCS/DACE CZ Office/2018_2019 whole years"
source("C:/Users/sk0007/OneDrive - Daikin Europe N.V/Documents/R scripts/DCS/DCS/MasterSctripts/Functions.R")
source("forecast_functions_for_DCS.R")
# load(file=file.path(working_path, "Data_structure.Rda"))

buildings_to_include <- c(118)
  # c(125, 123, 118, 116, 113, 111, 109, 108, 107, 102, 97) #c(123, 108, 109, 96, 44)
building_data_files_rows <- slice(building_data_files, buildings_to_include)

# report_file_name <- paste(building_data_files_row$Building,building_data_files_row$calc_timestamp)

```


```{r Data preparation}

# data preparation----
buildings_lst <- map(building_data_files_rows$bin_data_file, function(datafile){
  # browser()
  cat("loading datafile", datafile, "\n")
  data_struct <- load(file= datafile)
  cat("preparing outdoor dfs\n")
  outdoor_df_lst <- possibly_prepare_df_fun(get(data_struct))
  cat("outdoor df ready!\n")
  return(outdoor_df_lst)
}) %>% 
  set_names(building_data_files_rows$Building)


save(buildings_lst, file = "buildings_lst.Rda")
# load(file = "buildings_lst.Rda")
imap(flatten(buildings_lst), ~ write_csv(.x, path = str_c("Processing/Einsteinova_out_", .y , ".csv")))

```

## Evaluating predictors

Try to find predictors
Correlation matrix

```{r Correlation matrix, include=TRUE}
# pair_plot <- total_inc_tec_tsbl %>% 
#   as_tibble() %>% 
#   filter(dummy.worktime ==1) %>%
#   select(-TimeStampPosixct) %>%
#   # select_if(is.numeric) %>% 
#   GGally::ggpairs(aes(color = OpeMode))
# 
# pair_plot

plots <- map(buildings_lst$Stein, ~ ggplot(data = .x, aes(x = Operation.control.mode, y = consumption))+
  geom_boxplot()+
  coord_flip())

cowplot::plot_grid(plotlist = plots)

map(buildings_lst$Stein, ~ as_tibble(.x) %>% 
  group_by(Operation.control.mode) %>% 
  count()) %>% 
  bind_rows() %>% 
  group_by(Operation.control.mode) %>% 
  summarise(count = sum(n)) %>% 
  ungroup() %>% 
  mutate(ratio = round(count/sum(count) * 100)) %>% 
  flextable() %>% 
  autofit()

```

Model

```{r model dataframe creation}
# 

outdoor_df_lst <- flatten(buildings_lst) %>% 
  compact()

init_results_df <- map_dfr(outdoor_df_lst, function (hourly_df) {distinct(hourly_df, UniqueIdentifier, UnitType) %>% 
    mutate(hourly_df = list(hourly_df))
})

results_df_Cooling <- add_mode_columns(init_results_df, "Cooling", as.formula(consumption ~ ambient + average_suction + Tes.mean.value + hd + my + wd))
# results_df_Heating <- add_mode_columns(init_results_df, "Heating", as.formula(consumption ~ ambient + average_suction + Tcs.mean.value + hd + my + wd))
results_df_Total <- add_mode_columns(results_df_Cooling, "Heating", as.formula(consumption ~ ambient + average_suction + Tcs.mean.value + hd + my + wd))

unnested_Total_df <- unnest_df_fun(results_df_Total)
# Cooling_tbl <- create_tbl_fun(unnest_df_fun(results_df_Cooling))
Total_tbl <- create_tbl_fun(unnested_Total_df)

save_as_image(Total_tbl, path = "Total_tbl.png")
save_as_html(padding(Total_tbl, padding=1), path = "Total_tbl.html")
browseURL("Total_tbl.png")
browseURL("Total_tbl.html")

# save_as_docx(Cooling_tbl, path = "Cooling_tbl.docx")
# browseURL("Cooling_tbl.docx")

save(results_df_Total, file = "results_df_Total.Rda")
# load(file = "results_df_Total.Rda")

```

```{r tidymodels approach}
library(tidymodels)
library(doParallel)

# spped up calculation
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)

# reuse the df from previous functions
base_df <- outdoor_df_lst[[1]] %>% 
  select(TimeStampPosixct, consumption, ambient, OpeMode, Operation.control.mode, UniqueIdentifier, UnitType, SystemIdentifier, ope_time_out, Tes.mean.value, Tcs.mean.value, average_setpoint, average_ope_time, average_suction)  

# split into training and testing datasets. Stratify by consumption
base_df_split <- rsample::initial_split(
  base_df, 
  prop = 3/4, 
  strata = consumption
)

# preprocessing "recipe"
preprocessing_recipe <- 
  recipes::recipe(consumption ~ ., data = training(base_df_split)) %>%
  # update roles
  update_role(UniqueIdentifier, UnitType, SystemIdentifier, new_role = "id variable") %>% 
  update_role(OpeMode, average_setpoint, average_ope_time , new_role = "exclude variable") %>% 
  update_role(TimeStampPosixct, new_role = "time") %>% 
  # convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %>%
  # combine low frequency factor levels
  recipes::step_other(Operation.control.mode, threshold = 0.01) %>%
  # remove no variance predictors which provide no predictive information 
  recipes::step_nzv(all_nominal()) %>%
  step_mutate(dummy.worktime_out = if_else(ope_time_out == 0, 0, 1), tow = hour(TimeStampPosixct) + wday(TimeStampPosixct,week_start =  getOption("lubridate.week.start", 1)) * 24) %>%
  step_dummy(Operation.control.mode, preserve = TRUE) %>% 
  prep() 

# splitting for cross validation
base_df_cv_folds <- 
  recipes::bake(
    preprocessing_recipe, 
    new_data = training(base_df_split)
  ) %>%  
  rsample::vfold_cv(v = 5)

# XGBoost model specification
xgboost_model <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = 1000,
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()
  ) %>%
    set_engine("xgboost", objective = "reg:squarederror")

# grid specification
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 10
  )

# define the workflow
xgboost_wf <- 
  workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(consumption ~ ambient + tow) #. - TimeStampPosixct -OpeMode -ope_time_out -average_setpoint)

# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf,
  resamples = base_df_cv_folds,
  grid = xgboost_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE)
)

xgboost_tuned %>%
  tune::show_best(metric = "rmse", maximize = FALSE)

# isolate the best performing hyperparameter values.  

xgboost_best_params <- xgboost_tuned %>%
  tune::select_best("rmse", maximize = FALSE)

# finalize model
xgboost_model_final <- xgboost_model %>% 
  finalize_model(xgboost_best_params)

# Evaluate Performance on Test Data

train_processed <- bake(preprocessing_recipe,  new_data = training(base_df_split))

model_fit <- xgboost_model_final %>%
  # fit the model on all the training data
  fit(
    formula = consumption ~ ambient + tow, # -TimeStampPosixct -OpeMode -ope_time_out -average_setpoint,
    data    = train_processed
  )

train_prediction <- model_fit %>% 
  # predict the sale prices for the training data
  predict(new_data = train_processed) %>%
  bind_cols(training(base_df_split))

xgboost_score_train <- 
  train_prediction %>%
  yardstick::metrics(consumption, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

xgboost_score_train

# And now for the test data:

test_processed  <- bake(preprocessing_recipe, new_data = testing(base_df_split))

test_prediction <- model_fit %>% 
  # use the training model fit to predict the test data
  predict(new_data = test_processed) %>%
  bind_cols(testing(base_df_split))

# measure the accuracy of our model using `yardstick`
xgboost_score <- 
  test_prediction %>%
  yardstick::metrics(consumption, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))

xgboost_score

test_prediction %>% 
  summarise(sum(.pred), sum(consumption))


# test saving
setback_df <- train_processed %>%
apply_eco_shift_setpoint(Tsopt_cool = Inf, Tsopt_heat = -Inf)

setback_prediction <- model_fit %>% 
  # use the training model fit to predict the test data
  predict(new_data = setback_df) %>%
  bind_cols(setback_df)

setback_prediction %>% 
  summarise(sum(.pred), sum(consumption))

library(RMV2.0)
rmv_input_data <- base_df %>% 
  tibble::as_tibble() %>% 
  rename(time = TimeStampPosixct, eload = consumption, Temp = ambient) %>% 
  mutate(time = as.character(time, format = "%m/%d/%y %H:%M"))
undebug(RMV2.0::gbm_baseline)
gbm_baseline_test <- RMV2.0::gbm_baseline(train_Data = rmv_input_data)

  

```

```{r RMV 2}

system_name <- path_sanitize(unique(outdoor_df_lst[[1]]$UniqueIdentifier))

export_df <- outdoor_df_lst[[1]] %>%
  as_tibble() %>% 
  select(TimeStampPosixct, consumption, ambient, average_suction, ope_time_out, Tes.mean.value ,Tcs.mean.value, hd, my, wd, contains("dummy")) %>% 
  rename(time = TimeStampPosixct, eload = consumption, Temp = ambient) %>% 
  mutate(time = as.character(time, format = "%m/%d/%y %H:%M")) %>% 
  select(time, eload, Temp, average_suction, dummy.Heating, dummy.Cooling)
  
  pre_installation_path <- file.path("C:/RMV2/Preinstallation/", system_name)
  if(!dir.exists(pre_installation_path)) dir.create(path = pre_installation_path)
  write_csv(export_df, path = str_c("C:/RMV2/Preinstallation/", system_name, "/extra_var.csv"))

export_setback_df  <- outdoor_df_lst[[1]] %>%
  as_tibble() %>% 
  select(OpeMode, TimeStampPosixct, consumption, ambient, average_setpoint, average_suction, ope_time_out, Tes.mean.value ,Tcs.mean.value, hd, my, wd, contains("dummy")) %>% 
  apply_eco_shift_setpoint(Tsopt_cool = Inf, Tsopt_heat = -Inf) %>% 
  rename(time = TimeStampPosixct, eload = consumption, Temp = ambient) %>% 
  mutate(time = as.character(time, format = "%m/%d/%y %H:%M")) %>% 
  select(-OpeMode, average_setpoint) %>% 
  select(time, eload, Temp, average_suction, dummy.Heating, dummy.Cooling)
  

 post_installation_path <- file.path("C:/RMV2/Postinstallation/", system_name)
  if(!dir.exists(post_installation_path)) dir.create(path = post_installation_path)
  write_csv(export_setback_df, path = str_c("C:/RMV2/Postinstallation/", system_name, "/extra_var_setback.csv"))

```

